{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8d514-1177-4c11-8f75-2a63098f2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "# pip install datasets\n",
    "# pip install peft\n",
    "# pip install transformers\n",
    "# pip install bitsandbytes-cuda110 bitsandbytes\n",
    "# pip install accelerate\n",
    "# you may need to restart your kernel for your notebook if you installed all of the above\n",
    "\n",
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df5952-30c2-4456-b171-13b663127c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_examples(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples['text'])\n",
    "    tokenized_inputs['labels'] = examples['labels']\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# define custom batch preprocessor\n",
    "def collate_fn(batch, tokenizer):\n",
    "    dict_keys = ['input_ids', 'attention_mask', 'labels']\n",
    "    d = {k: [dic[k] for dic in batch] for k in dict_keys}\n",
    "    d['input_ids'] = torch.nn.utils.rnn.pad_sequence(\n",
    "        d['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    d['attention_mask'] = torch.nn.utils.rnn.pad_sequence(\n",
    "        d['attention_mask'], batch_first=True, padding_value=0\n",
    "    )\n",
    "    d['labels'] = torch.stack(d['labels'])\n",
    "    return d\n",
    "\n",
    "\n",
    "# define which metrics to compute for evaluation\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    f1_micro = f1_score(labels, predictions > 0, average = 'micro')\n",
    "    f1_macro = f1_score(labels, predictions > 0, average = 'macro')\n",
    "    f1_weighted = f1_score(labels, predictions > 0, average = 'weighted')\n",
    "    return {\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted\n",
    "    }\n",
    "\n",
    "# create custom trainer class to be able to pass label weights and calculate mutilabel loss\n",
    "class CustomTrainer(Trainer):\n",
    "\n",
    "    def __init__(self, label_weights, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.label_weights = label_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # compute custom loss\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels.to(torch.float32), pos_weight=self.label_weights)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# set random seed\n",
    "random.seed(0)\n",
    "\n",
    "# load data\n",
    "with open('train.csv', newline='') as csvfile:\n",
    "    data = list(csv.reader(csvfile, delimiter=','))\n",
    "    header_row = data.pop(0)\n",
    "\n",
    "# shuffle data\n",
    "random.shuffle(data)\n",
    "\n",
    "# reshape\n",
    "idx, text, labels = list(zip(*[(int(row[0]), f'Title: {row[1].strip()}\\n\\nAbstract: {row[2].strip()}', row[3:]) for row in data]))\n",
    "labels = np.array(labels, dtype=int)\n",
    "\n",
    "# create label weights\n",
    "label_weights = 1 - labels.sum(axis=0) / labels.sum()\n",
    "\n",
    "# stratified train test split for multilabel ds\n",
    "row_ids = np.arange(len(labels))\n",
    "train_idx, y_train, val_idx, y_val = iterative_train_test_split(row_ids[:,np.newaxis], labels, test_size = 0.1)\n",
    "x_train = [text[i] for i in train_idx.flatten()]\n",
    "x_val = [text[i] for i in val_idx.flatten()]\n",
    "\n",
    "# create hf dataset\n",
    "ds = DatasetDict({\n",
    "    'train': Dataset.from_dict({'text': x_train, 'labels': y_train}),\n",
    "    'val': Dataset.from_dict({'text': x_val, 'labels': y_val})\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044be56-fa0e-4692-b25c-b8701881f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "model_name = 'tiiuae/falcon-rw-1b'\n",
    "\n",
    "# preprocess dataset with tokenizer\n",
    "def tokenize_examples(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples['text'])\n",
    "    tokenized_inputs['labels'] = examples['labels']\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_ds = ds.map(functools.partial(tokenize_examples, tokenizer=tokenizer), batched=True)\n",
    "tokenized_ds = tokenized_ds.with_format('torch')\n",
    "\n",
    "# qunatization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # enable 4-bit quantization\n",
    "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    ")\n",
    "\n",
    "# lora config\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, # the dimension of the low-rank matrices\n",
    "    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    #target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'], # for mistral models\n",
    "    target_modules = [\"query_key_value\"], # for falcon\n",
    "    lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
    "    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "    task_type = 'SEQ_CLS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c7b60-daee-494c-89ea-f3bcd86f38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=labels.shape[1]\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b67fd0-c897-4a7a-8636-913362d943c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of where trained model will be housed\n",
    "output_dir = 'falcon_1B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f736df-88eb-4e47-bd2d-ada8632e0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 8, # you may need to make this smaller to run\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 2,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_ds['train'],\n",
    "    eval_dataset = tokenized_ds['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = functools.partial(collate_fn, tokenizer=tokenizer),\n",
    "    compute_metrics = compute_metrics,\n",
    "    label_weights = torch.tensor(label_weights, device=model.device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b35f1e-c01f-47d6-8558-5cc439826d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ce589-834f-413f-9335-8c3286080586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "peft_model_id = output_dir\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n",
    "\n",
    "# load model\n",
    "peft_model_id = output_dir\n",
    "model = AutoModelForSequenceClassification.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600a22c-951f-46e1-88d8-4c396037ff71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0a838-61a2-4585-9db5-03578f959d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec00fb-9407-4132-86e2-d0d4b0f9025d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
